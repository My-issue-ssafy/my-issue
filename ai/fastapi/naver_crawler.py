# -*- coding: utf-8 -*-
"""
Naver ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ì™„ì„±ë³¸ / ë³¸ë¬¸ ìœ„ì¹˜ê¸°ë°˜ ì´ë¯¸ì§€ ì¸ë¼ì¸ + KO-BERT ì œëª© ì„ë² ë”© ì €ì¥)

ì´ í¬ë¡¤ëŸ¬ëŠ” ë„¤ì´ë²„ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
- ê¸°ì‚¬ ë³¸ë¬¸ì€ blocks/markdown ë‘ ê°€ì§€ ëª¨ë“œ ì¤‘ ì„ íƒ ê°€ëŠ¥
- ì €ì¥: ë°œí–‰ì¼ ê¸°ì¤€ JSONL (output/YYYYMMDD.jsonl)
- ì¤‘ë³µ ì œê±°: ê¸°ì‚¬(oid/aid) ê¸°ì¤€
- ì•ˆì •ì„±: Session+Retry, Referer, ë¬´ì‘ìœ„ ìŠ¬ë¦½, í˜ì´ì§€ HTML í•´ì‹œ ë¹„êµ
- [NEW] ê¸°ì‚¬ë‹¹ ì œëª©ì„ KoBERT(768ì°¨ì›)ë¡œ ì„ë² ë”©í•´ì„œ JSONLì— í•¨ê»˜ ì €ì¥

í•„ìˆ˜ íŒ¨í‚¤ì§€(ìµœì´ˆ 1íšŒ):
pip install transformers torch sentencepiece
"""

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os, re, json, time, hashlib, random
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin, urlparse, parse_qs, unquote

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from bs4.element import NavigableString, Tag

# ========================
# ì „ì—­ ì„¤ì • ë³€ìˆ˜ë“¤
# ========================

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/127.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}

MAX_PAGES = 1
CRAWL_BACK_DAYS = 1
LIST_DELAY = (0.4, 0.9)
DETAIL_DELAY = (0.6, 1.3)

INLINE_IMAGES_MODE = "blocks"  # "markdown" ê°€ëŠ¥

# ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ ì„¹ì…˜ ì½”ë“œ (sid1)
NAVER_SECTIONS = ["100", "101", "102", "103", "104", "105"]

SEARCH_KEYWORDS = [
    "ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "êµ­ì œ", "ë¬¸í™”", "IT", "ê³¼í•™", "ì‚¬ê±´", "ì‚¬ê³ ", "ë°œí‘œ", "ê²°ì •",
]

PC_LIST_WRAPPERS = [
    "div#main_content div.list_body",
    "div#main_content ul.type06_headline",
    "div#main_content ul.type06",
    "div#main_content ul",
]

ARTICLE_ID_RE = re.compile(r"(?:https?://n\.news\.naver\.com)?/article/(\d{3})/(\d{9,10})(?:[?#].*)?$")
READ_QUERY_RE = re.compile(r"(?:https?://news\.naver\.com)?/main/read\.naver")

IMAGE_EXTS = (".jpg", ".jpeg", ".png", ".gif", ".webp", ".bmp", ".svg", ".avif", ".heic")
IMG_URL_RE = re.compile(
    r"https?://[^\s\"'>)]+?(?:"
    r"\.jpg|\.jpeg|\.png|\.gif|\.webp|\.bmp|\.svg|\.avif|\.heic)"
    r"(?:\?[^\s\"'>)]*)?", re.IGNORECASE
)

DOWNLOAD_IMAGES = False
IMAGES_DIR = "images"

# ì„¹ì…˜ í—ˆìš© ëª©ë¡ (ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í† í°ë§Œ ì±„íƒ)
ALLOWED_SECTIONS = ["ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ìƒí™œ", "ë¬¸í™”", "ì„¸ê³„", "IT", "ê³¼í•™"]

# ========================
# [NEW] KO-BERT ì œëª© ì„ë² ë”© ì„¤ì •/ìœ í‹¸
# ========================

EMBEDDING_ENABLED = True
EMBED_MODEL_NAME = "monologg/kobert"  # 768-dim

_KOBERT_TOKENIZER = None
_KOBERT_MODEL = None
_KOBERT_DEVICE = None

def _get_kobert():
    """
    ì „ì—­ 1íšŒ ë¡œë”©: í† í¬ë‚˜ì´ì €/ëª¨ë¸/ë””ë°”ì´ìŠ¤
    """
    global _KOBERT_TOKENIZER, _KOBERT_MODEL, _KOBERT_DEVICE
    if _KOBERT_MODEL is None or _KOBERT_TOKENIZER is None:
        from transformers import AutoTokenizer, AutoModel
        import torch
        _KOBERT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        # ğŸ‘‡ ì—¬ê¸° trust_remote_code=True ì¶”ê°€
        _KOBERT_TOKENIZER = AutoTokenizer.from_pretrained(
            EMBED_MODEL_NAME,
            trust_remote_code=True
        )
        _KOBERT_MODEL = AutoModel.from_pretrained(
            EMBED_MODEL_NAME,
            trust_remote_code=True
        ).to(_KOBERT_DEVICE)
        _KOBERT_MODEL.eval()
        print(f"[EMB] loaded: {EMBED_MODEL_NAME} on {_KOBERT_DEVICE}")
    return _KOBERT_TOKENIZER, _KOBERT_MODEL, _KOBERT_DEVICE

def embed_title(text: str):
    """
    ì œëª© â†’ 768ì°¨ì› KoBERT ì„ë² ë”©(list[float])
    - mean-pooling + L2 ì •ê·œí™”(ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
    """
    if not EMBEDDING_ENABLED or not text:
        return None
    from torch.nn.functional import normalize
    import torch
    tok, mdl, device = _get_kobert()
    inputs = tok(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    ).to(device)
    with torch.no_grad():
        last_hidden = mdl(**inputs).last_hidden_state   # [1, L, 768]
    mask = inputs["attention_mask"].unsqueeze(-1)       # [1, L, 1]
    vec = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)  # mean-pooling
    vec = normalize(vec, p=2, dim=1)                    # L2 ì •ê·œí™”
    return vec.squeeze(0).cpu().tolist()                # 768 floats

# ========================
# HTTP ì„¸ì…˜ ë° ì•ˆì „í•œ GET
# ========================

def make_session() -> requests.Session:
    s = requests.Session()
    retries = Retry(
        total=3,
        connect=3,
        read=3,
        backoff_factor=0.7,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD","GET","OPTIONS"]
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    s.headers.update(HEADERS)
    return s

SESSION = make_session()

def rand_sleep(bounds):
    lo, hi = bounds
    time.sleep(random.uniform(lo, hi))

def safe_get(url: str, timeout: float = 15.0, extra_retries: int = 2, referer: str | None = None):
    last_exc = None
    for i in range(extra_retries + 1):
        try:
            headers = HEADERS.copy()
            if referer:
                headers["Referer"] = referer
            headers["Cache-Control"] = "no-cache"
            headers["Pragma"] = "no-cache"
            return SESSION.get(url, timeout=timeout, headers=headers)
        except requests.RequestException as e:
            last_exc = e
            sleep = (2 ** i) * 0.7 + random.random() * 0.3
            print(f"[NET-RETRY] {url} -> {e} | retry in {sleep:.1f}s")
            time.sleep(sleep)
    raise last_exc

# ========================
# ìœ í‹¸ë¦¬í‹°
# ========================

def yyyymmdd_from_iso(iso_str: str | None) -> str | None:
    if not iso_str:
        return None
    s = iso_str.strip()
    if re.fullmatch(r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", s):
        return s[:10].replace("-", "")
    if re.fullmatch(r"\d{14}", s):
        return s[:8]
    digits = re.findall(r"\d", s)
    return "".join(digits[:8]) if len(digits) >= 8 else None

def append_jsonl(path: str, item: dict):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

def _abs_url(base: str, u: str) -> str:
    if not u:
        return ""
    if u.startswith("//"):
        return "https:" + u
    return urljoin(base, u)

def canonicalize_article_url(u: str) -> tuple[str | None, tuple[str, str] | None]:
    if not u:
        return None, None
    m = ARTICLE_ID_RE.search(u)
    if m:
        oid, aid = m.group(1), m.group(2)
        return f"https://n.news.naver.com/article/{oid}/{aid}", (oid, aid)
    if READ_QUERY_RE.search(u):
        pr = urlparse(u)
        qs = parse_qs(pr.query)
        oid = (qs.get("oid") or [""])[0]
        aid = (qs.get("aid") or [""])[0]
        if re.fullmatch(r"\d{3}", oid) and re.fullmatch(r"\d{9,10}", aid):
            return f"https://n.news.naver.com/article/{oid}/{aid}", (oid, aid)
    return None, None

# ========================
# ë³¸ë¬¸ íŒŒì‹± ë³´ì¡°
# ========================

COPYRIGHT_RE = re.compile(r"(ë¬´ë‹¨ì „ì¬|ì¬ë°°í¬ ê¸ˆì§€|ì €ì‘ê¶Œ)")

def _normalize_img_url(u: str, base_url: str) -> str | None:
    if not u:
        return None
    u = _abs_url(base_url, u.strip())
    core = u.split("?", 1)[0].lower()
    if not core.endswith(IMAGE_EXTS):
        if "/image/" in core or "/img/" in core:
            return u
        return None
    return u

# def _text_clean(s: str) -> str:
#     s = re.sub(r"\s+", " ", s).strip()
#     if not s:
#         return ""
#     if COPYRIGHT_RE.search(s):
#         return ""
#     return s

def _text_clean(s: str) -> str:
    # \nì€ ì‚´ë¦¬ê³ , ë‚˜ë¨¸ì§€ ì—°ì† ê³µë°±ë§Œ ì¶•ì†Œ
    s = re.sub(r"[ \t]+", " ", s)   # ìŠ¤í˜ì´ìŠ¤/íƒ­ë§Œ ì •ë¦¬
    s = re.sub(r"\n{3,}", "\n\n", s)  # ì¤„ë°”ê¿ˆì´ 3ê°œ ì´ìƒ â†’ 2ê°œ
    s = s.strip()
    if not s:
        return ""
    if COPYRIGHT_RE.search(s):
        return ""
    return s


def _body_root_candidates(soup: BeautifulSoup):
    cands = [
        "#newsct_article",
        "article#dic_area",
        "div#articeBody",
        "div#articleBody",
        "div#newsEndContents",
    ]
    for sel in cands:
        node = soup.select_one(sel)
        if node:
            return node
    return soup

# --- blocks ëª¨ë“œ
def build_body_blocks(soup: BeautifulSoup, base_url: str) -> list[dict]:
    root = _body_root_candidates(soup)
    blocks = []
    buf = []

    def flush_buf():
        nonlocal buf
        txt = _text_clean("".join(buf))
        if txt:
            blocks.append({"type": "text", "content": txt})
        buf = []

    def walk(node: Tag | NavigableString):
        if isinstance(node, NavigableString):
            buf.append(str(node))
            return
        if not isinstance(node, Tag):
            return
        name = (node.name or "").lower()
        if name in ("style", "script", "noscript", "iframe"):
            return
        if name in ("br",):
            buf.append("\n")
        if name in ("img",):
            u = node.get("data-src") or node.get("src") or node.get("data-origin")
            u = _normalize_img_url(u, base_url)
            if u:
                flush_buf()
                blocks.append({"type": "image", "content": u})
            return
        if name in ("picture", "figure"):
            for child in node.children:
                walk(child)
            return
        cls = " ".join(node.get("class") or [])
        if any(key in cls for key in ["_VOD_PLAYER_WRAP", "as_addinfo", "media_end_linked", "promotion"]):
            return
        for child in node.children:
            walk(child)
        if name in ("p", "div", "li", "section", "article", "blockquote"):
            buf.append("\n")

    for child in root.children:
        walk(child)
    flush_buf()

    cleaned = []
    for b in blocks:
        if b["type"] == "text":
            t = b["content"].strip()
            if not t:
                continue
            cleaned.append({"type": "text", "content": t})
        else:
            cleaned.append(b)
    return cleaned

def build_body_markdown(soup: BeautifulSoup, base_url: str) -> str | None:
    blocks = build_body_blocks(soup, base_url)
    parts = []
    for b in blocks:
        if b["type"] == "text":
            parts.append(b["content"])
        else:
            parts.append(f"![]({b['content']})")
    md = "\n\n".join(parts).strip()
    return md or None

def build_body_text(soup: BeautifulSoup) -> str | None:
    root = _body_root_candidates(soup)
    ps = root.select("p")
    out = []
    for p in ps:
        txt = _text_clean(p.get_text(" ", strip=True))
        if txt:
            out.append(txt)
    return "\n".join(out).strip() or None

# --- ë©”íƒ€ íŒŒì‹±
def parse_title(soup: BeautifulSoup) -> str | None:
    h = soup.select_one("h2.media_end_head_headline, #newsct_article h2")
    if h:
        t = h.get_text(" ", strip=True)
        if t:
            return t
    ogt = soup.find("meta", {"property": "og:title"})
    return ogt["content"].strip() if ogt and ogt.get("content") else None

def parse_regdate_iso(soup: BeautifulSoup) -> str | None:
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or tag.text or "")
            items = data if isinstance(data, list) else [data]
            for it in items:
                if not isinstance(it, dict):
                    continue
                t = it.get("@type") or it.get("@type".lower())
                if (t == "NewsArticle") or ("Article" in str(t)):
                    dp = (it.get("datePublished") or it.get("dateCreated") or it.get("uploadDate"))
                    if dp:
                        dp = str(dp).strip().replace(" ", "T")
                        return dp.split("+")[0].split("Z")[0]
        except Exception:
            continue
    for key in [
        {"property": "article:published_time"},
        {"name": "date"},
        {"name": "publication_date"},
        {"itemprop": "datePublished"},
    ]:
        m = soup.find("meta", key)
        if m and m.get("content"):
            iso = m["content"].strip().replace(" ", "T")
            return iso.split("+")[0].split("Z")[0]
    t = soup.select_one("span.media_end_head_info_datestamp_time, time")
    if t:
        for attr in ["datetime", "data-date-time", "data-date", "data-datetime"]:
            v = t.get(attr)
            if v:
                v = v.strip().replace(" ", "T")
                return v.split("+")[0].split("Z")[0]
        txt = t.get_text(" ", strip=True)
        digits = re.findall(r"\d", txt)
        if len(digits) >= 12:
            y = "".join(digits[:4]); M = "".join(digits[4:6]); d = "".join(digits[6:8])
            h = "".join(digits[8:10]); mi = "".join(digits[10:12])
            return f"{y}-{M}-{d}T{h}:{mi}:00"
    return None

def parse_press(soup: BeautifulSoup) -> str | None:
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or tag.text or "")
            items = data if isinstance(data, list) else [data]
            for it in items:
                if not isinstance(it, dict):
                    continue
                pub = it.get("publisher")
                if isinstance(pub, dict):
                    name = pub.get("name")
                    if name:
                        return str(name).strip()
        except Exception:
            continue
    logo_img = soup.select_one("a.media_end_head_top_logo img")
    if logo_img and logo_img.get("alt"):
        return logo_img["alt"].strip()
    auth = soup.find("meta", {"property": "og:article:author"}) or soup.find("meta", {"name": "author"})
    if auth and auth.get("content"):
        return auth["content"].strip()
    return None

# í”í•œ ì„±ì”¨ ì§‘í•©
_SURNAMES_1 = set(list("ê¹€ì´ë°•ìµœì •ê°•ì¡°ìœ¤ì¥ì„ì˜¤í•œì‹ ì„œê¶Œí™©ì•ˆì†¡ì „í™ìœ ê³ ë¬¸ì–‘ì†ë°°ë°±í—ˆë‚¨ì‹¬ë…¸í•˜ê³½ì„±ì°¨ì£¼ìš°êµ¬ë¯¼ë¥˜ì§„ì—„ì±„ì›ì²œë°©ë³€í•¨ì—¼ì—¬ìœ¡ì„íƒë‘ë¹ˆë™ì˜¨ìœ„í‘œê¸°ë°˜ìš©ë§ˆì™•"))
_SURNAMES_2 = {"ë‚¨ê¶", "ì œê°ˆ", "ì„ ìš°", "í™©ë³´", "ì‚¬ê³µ", "ì„œë¬¸", "ë…ê³ "}

def _looks_like_korean_person_name(name: str) -> bool:
    if not name:
        return False
    n = re.sub(r"\s+", "", str(name))
    noise_words = {"ëŒ€í†µë ¹ì‹¤", "ê¸°ìë‹¨", "ì‚¬ì§„ê¸°ì", "ì˜ìƒ", "ì œê³µ", "ì—°ê²°", "ë³´ë„êµ­", "ë°ìŠ¤í¬"}
    if n in noise_words:
        return False
    if not re.fullmatch(r"[ê°€-í£]+", n):
        return False
    for fx in _SURNAMES_2:
        if n.startswith(fx) and re.fullmatch(r"[ê°€-í£]{3,4}", n):
            return True
    if n[0] not in _SURNAMES_1:
        return False
    return bool(re.fullmatch(r"[ê°€-í£]{2,4}", n))

def parse_reporter(soup: BeautifulSoup) -> str | None:
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or tag.text or "")
            items = data if isinstance(data, list) else [data]
            for it in items:
                if not isinstance(it, dict):
                    continue
                author = it.get("author")
                if not author:
                    continue
                cand_list = author if isinstance(author, list) else [author]
                for a in cand_list:
                    if isinstance(a, dict):
                        name = str(a.get("name") or "").replace(" ê¸°ì", "").strip()
                        atype = str(a.get("@type") or a.get("type") or "").lower()
                        if (_looks_like_korean_person_name(name) and (atype in {"person", ""})):
                            return name
                    elif isinstance(a, str):
                        nm = a.replace(" ê¸°ì", "").strip()
                        if _looks_like_korean_person_name(nm):
                            return nm
        except Exception:
            pass

    cands = [
        "em.media_end_head_journalist_name",
        ".journalist_name", ".reporter_name", ".author", ".byline",
        ".journalistcard_summary_name",
        "span.byline_s", "div.byline span", "div.byline", "p.byline_p",
        'meta[name="author"]',
    ]
    for sel in cands:
        el = soup.select_one(sel)
        if not el:
            continue
        txt = (el.get("content") if getattr(el, "name", "") == "meta" else el.get_text(" ", strip=True)) or ""
        if not txt:
            continue
        txt = re.sub(r"[\[\]()<>{}ã€ˆã€‰]", " ", txt)
        txt = re.sub(r"\S+@\S+", " ", txt)
        txt = re.sub(r"\s+", " ", txt).strip()
        if "ê¸°ì" not in txt and "@" not in txt:
            continue
        m = re.search(r"([ê°€-í£]{2,4})\s*(?:[ê°€-í£A-Za-zÂ·\-/]{0,12}\s*)?ê¸°ì(?![ê°€-í£])", txt)
        if m:
            name = m.group(1)
            if _looks_like_korean_person_name(name):
                return name

    full = soup.get_text(" ", strip=True)
    for inner in re.findall(r"\[([^\]]]{2,80})\]", full):
        inner = re.sub(r"\S+@\S+", " ", inner)
        m = re.search(r"([ê°€-í£]{2,4})\s*(?:[ê°€-í£A-Za-zÂ·\-/]{0,12}\s*)?ê¸°ì(?![ê°€-í£])", inner)
        if m and _looks_like_korean_person_name(m.group(1)):
            return m.group(1)

    m = re.search(r"([ê°€-í£]{2,4})\s*(?:[ê°€-í£A-Za-zÂ·\-/]{0,12}\s*)?(?:ê¸°ì|ê¸°ì=)?\s*[A-Za-z0-9._%+-]+@", full)
    if m and _looks_like_korean_person_name(m.group(1)):
        return m.group(1)

    for p in [
        r"([ê°€-í£]{2,4})\s*(?:[ê°€-í£A-Za-zÂ·\-/]{0,12}\s*)?ê¸°ì\s*=",
        r"([ê°€-í£]{2,4})\s*(?:[ê°€-í£A-Za-zÂ·\-/]{0,12}\s*)?ê¸°ì(?![ê°€-í£])",
    ]:
        m = re.search(p, full)
        if m and _looks_like_korean_person_name(m.group(1)):
            return m.group(1)
    return None

# ========================
# ì„¹ì…˜ íŒŒì‹± (ì •í™• ì¼ì¹˜ 8ê°œ, ì—¬ëŸ¬ ê°œë©´ ë¦¬ìŠ¤íŠ¸)
# ========================

_SECTION_SEPS_RE = re.compile(r"[ï¼/Â·ã†,ï¼Œ\|\>]+|\s*,\s*")

def parse_sections(soup: BeautifulSoup) -> list[str]:
    """
    ê¸°ì‚¬ì˜ ì„¹ì…˜ ë¼ë²¨ì„ 8ê°œ í—ˆìš© ì§‘í•©ì—ì„œ 'ì •í™•íˆ ì¼ì¹˜'í•˜ëŠ” ê°’ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    ìš°ì„ ìˆœìœ„:
      1) ë³¸ë¬¸ ì „ì—­ í…ìŠ¤íŠ¸ì˜ ë¬¸ì¥í˜• ë¼ë²¨: "ì´ ê¸°ì‚¬ëŠ” ì–¸ë¡ ì‚¬ì—ì„œ {â€¦} ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤"
      2) JSON-LD articleSection/section (ë‹¨ì¼/ë°°ì—´/@graph/mainEntity)
      3) meta(article:section / name=section)
      4) breadcrumb/ì„¹ì…˜ ë§í¬ í…ìŠ¤íŠ¸
    êµ¬ë¶„ì: / ï¼ Â· ã† , ï¼Œ | > (ë° ê³µë°±ì½¤ë§ˆ)
    """
    allowed = set(ALLOWED_SECTIONS)
    candidates: list[str] = []

    # 1) ë¬¸ì¥í˜• ë¼ë²¨
    full_text = soup.get_text(" ", strip=True)
    m = re.search(r"ì´ ê¸°ì‚¬ëŠ” ì–¸ë¡ ì‚¬ì—ì„œ\s*(.+?)\s*ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤", full_text)
    if m:
        candidates.append(m.group(1))

    # 2) JSON-LD
    def _extract_article_sections_from_ld(ld):
        secs = []
        if isinstance(ld, dict):
            if ld.get("@type") in ("NewsArticle", "Article"):
                v = ld.get("articleSection") or ld.get("section")
                if v:
                    if isinstance(v, (list, tuple)):
                        secs.extend([str(x).strip() for x in v if str(x).strip()])
                    else:
                        secs.append(str(v).strip())
            me = ld.get("mainEntity")
            if isinstance(me, (list, dict)):
                secs.extend(_extract_article_sections_from_ld(me) or [])
            g = ld.get("@graph")
            if isinstance(g, list):
                for it in g:
                    secs.extend(_extract_article_sections_from_ld(it) or [])
        elif isinstance(ld, list):
            for it in ld:
                secs.extend(_extract_article_sections_from_ld(it) or [])
        return [s for s in secs if s]

    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            raw = tag.string or tag.text or ""
            if not raw.strip():
                continue
            data = json.loads(raw)
            candidates.extend(_extract_article_sections_from_ld(data))
        except Exception:
            continue

    # 3) ë©”íƒ€
    for key in [
        {"property": "article:section"},
        {"name": "section"},
        {"name": "article:section"},
        {"property": "og:section"},
    ]:
        meta = soup.find("meta", key)
        if meta and meta.get("content"):
            candidates.append(meta["content"].strip())

    # 4) breadcrumb/ì„¹ì…˜ ë§í¬
    for sel in [
        ".media_end_categorize a",
        ".media_end_head_top .media_end_categorize_item",
        "nav a[aria-current='page']",
        "nav a[href*='/section/']",
    ]:
        for a in soup.select(sel):
            t = a.get_text(" ", strip=True)
            if t:
                candidates.append(t)

    # í† í°í™” â†’ í—ˆìš© ì§‘í•©ê³¼ ì •í™• ì¼ì¹˜ë§Œ ìœ ì§€ (ì¤‘ë³µ ì œê±°, ìµœì´ˆ ë“±ì¥ ìˆœì„œ ìœ ì§€)
    seen = set()
    out: list[str] = []
    for cand in candidates:
        for tok in _SECTION_SEPS_RE.split(str(cand)):
            tok = tok.strip()
            if tok in allowed and tok not in seen:
                seen.add(tok)
                out.append(tok)

    return out  # ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸

# ========================
# ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ë° íŒŒì‹±
# ========================

def fetch_and_parse(url: str) -> dict | None:
    canon_url, id_tuple = canonicalize_article_url(url)
    target = canon_url or url
    r = safe_get(target, timeout=15.0, referer="https://news.naver.com/")
    if r.status_code != 200:
        return None
    soup = BeautifulSoup(r.text, "lxml")

    item: dict = {
        "url": canon_url or target,
        "title": parse_title(soup),
        "published_at": parse_regdate_iso(soup),
        "press": parse_press(soup),
        "reporter": parse_reporter(soup),
        "section": parse_sections(soup),  # ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ (ì •í™• ì¼ì¹˜ 8ê°œë§Œ)
        "crawled_at": datetime.now(timezone.utc).isoformat(),
    }

    if INLINE_IMAGES_MODE == "blocks":
        item["body"] = build_body_blocks(soup, base_url=item["url"])
    else:
        item["body"] = build_body_markdown(soup, base_url=item["url"])

    if DOWNLOAD_IMAGES and isinstance(item.get("body"), list):
        oid, aid = (id_tuple or ("", ""))
        article_key = f"{oid}_{aid}" if oid and aid else hashlib.md5(item["url"].encode("utf-8")).hexdigest()[:10]
        pub_yyyymmdd = yyyymmdd_from_iso(item.get("published_at")) or "unknown"
        idx = 0
        for b in item["body"]:
            if b["type"] == "image":
                idx += 1
                iu = b["content"]
                ext = os.path.splitext(iu.split("?", 1)[0])[1] or ".jpg"
                local_path = os.path.join(IMAGES_DIR, pub_yyyymmdd, f"{article_key}_{idx:02d}{ext}")
                try:
                    os.makedirs(os.path.dirname(local_path), exist_ok=True)
                    rr = safe_get(iu, timeout=20, referer=item["url"])
                    if rr.status_code == 200 and rr.content:
                        with open(local_path, "wb") as f:
                            f.write(rr.content)
                except Exception as e:
                    print("[IMG-ERR]", iu, e)

    return item

# ========================
# ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
# ========================

def extract_links_from_pc_html(html: str) -> list[str]:
    soup = BeautifulSoup(html, "lxml")
    hrefs: set[str] = set()
    wrappers = []
    for sel in PC_LIST_WRAPPERS:
        wrappers.extend(soup.select(sel))
    for w in wrappers:
        for a in w.find_all("a", href=True):
            hrefs.add(a["href"].strip())
    if not hrefs:
        wrapper = soup.select_one("#main_content")
        if wrapper:
            for a in wrapper.find_all("a", href=True):
                hrefs.add(a["href"].strip())
    if not hrefs:
        for a in soup.find_all("a", href=True):
            hrefs.add(a["href"].strip())
    canon: set[str] = set()
    for h in hrefs:
        cu, _id = canonicalize_article_url(h)
        if cu and _id:
            canon.add(cu)
    return list(canon)

def discover_links(sid1: str, reg_date: str, max_pages: int = MAX_PAGES) -> list[str]:
    found: set[str] = set()
    same_html_streak = 0
    prev_hash = None
    for page in range(1, max_pages + 1):
        list_url = f"https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1={sid1}&date={reg_date}&page={page}"
        try:
            ref = f"https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1={sid1}&date={reg_date}&page={page-1}" if page > 1 \
                else f"https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1={sid1}&date={reg_date}"
            r = safe_get(list_url, timeout=12.0, referer=ref)
        except Exception as e:
            print(f"[ERR] discover sid1={sid1}/{reg_date} p{page} -> {e}")
            break
        if r.status_code != 200:
            print(f"[WARN] {list_url} -> {r.status_code}")
            break
        html = r.text
        h = hashlib.sha1(html.encode("utf-8", "ignore")).hexdigest()
        new_links = extract_links_from_pc_html(html)
        before = len(found)
        for u in new_links:
            found.add(u)
        page_new = len(found) - before
        print(f"[sid1:{sid1}][{reg_date}][p{page}] so_far={len(found)} page_new={page_new} hash={h[:8]}")
        if prev_hash == h or page_new == 0:
            same_html_streak += 1
        else:
            same_html_streak = 0
        prev_hash = h
        if same_html_streak >= 2:
            break
        rand_sleep(LIST_DELAY)
    return list(found)

def discover_links_from_main_page() -> list[str]:
    found: set[str] = set()
    main_urls = [
        "https://news.naver.com/",
        "https://news.naver.com/section/100",
        "https://news.naver.com/section/101",
        "https://news.naver.com/section/102",
        "https://news.naver.com/section/103",
        "https://news.naver.com/section/104",
        "https://news.naver.com/section/105",
        "https://news.naver.com/main/ranking/popularDay.naver",
    ]
    for url in main_urls:
        try:
            r = safe_get(url, timeout=12.0, referer="https://news.naver.com/")
            if r.status_code == 200:
                for u in extract_links_from_pc_html(r.text):
                    found.add(u)
                print(f"[MAIN-PAGE] {url} -> cumulative {len(found)}")
        except Exception as e:
            print(f"[ERR] main page {url} -> {e}")
        rand_sleep(LIST_DELAY)
    return list(found)

def discover_links_by_keyword(keyword: str, max_pages: int = 5) -> list[str]:
    found: set[str] = set()
    same_html_streak = 0
    prev_hash = None
    for page in range(1, max_pages + 1):
        search_url = f"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={keyword}&start={(page-1)*10+1}"
        try:
            r = safe_get(search_url, timeout=12.0, referer="https://search.naver.com/")
        except Exception as e:
            print(f"[ERR] keyword {keyword} p{page} -> {e}")
            break
        if r.status_code != 200:
            break
        html = r.text
        h = hashlib.sha1(html.encode("utf-8", "ignore")).hexdigest()
        soup = BeautifulSoup(html, "lxml")
        hrefs = set(a["href"].strip() for a in soup.find_all("a", href=True))
        before = len(found)
        for hrf in hrefs:
            cu, _id = canonicalize_article_url(hrf)
            if cu and _id:
                found.add(cu)
        page_new = len(found) - before
        print(f"[KEY:{keyword}][p{page}] so_far={len(found)} page_new={page_new} hash={h[:8]}")
        if prev_hash == h or page_new == 0:
            same_html_streak += 1
        else:
            same_html_streak = 0
        prev_hash = h
        if same_html_streak >= 2:
            break
        rand_sleep(LIST_DELAY)
    return list(found)

# ========================
# ì €ì¥ í—¬í¼(oid/aid ê¸°ì¤€)
# ========================
def save_item_if_new(item: dict, seen_ids: set[str]) -> bool:
    cu, id_tuple = canonicalize_article_url(item.get("url") or "")
    if not id_tuple:
        return False
    oid, aid = id_tuple
    key = f"{oid}/{aid}"
    if key in seen_ids:
        return False
    pub_yyyymmdd = yyyymmdd_from_iso(item.get("published_at"))
    out_path = os.path.join("output", f"{pub_yyyymmdd}.jsonl") if pub_yyyymmdd else os.path.join("output", "unknown.jsonl")
    append_jsonl(out_path, item)
    seen_ids.add(key)
    return True

# ========================
# ë©”ì¸
# ========================
if __name__ == "__main__":
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìˆ˜ì§‘ ì „ìš©)
    """
    today = datetime.today()
    reg_dates = [(today - timedelta(days=i)).strftime("%Y%m%d") for i in range(CRAWL_BACK_DAYS)]
    seen_ids: set[str] = set()

    print(f"â–¶ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì „ìš© ìˆ˜ì§‘ ì‹œì‘: days={CRAWL_BACK_DAYS}, max_pages={MAX_PAGES}, sections={len(NAVER_SECTIONS)}")

    for reg in reg_dates:
        print(f"\n=== ë‚ ì§œ: {reg} (list-only) ===")
        for sid1 in NAVER_SECTIONS:
            try:
                urls = discover_links(sid1, reg, max_pages=MAX_PAGES)
            except KeyboardInterrupt:
                raise
            except Exception as e:
                print(f"[ERR] discover sid1={sid1}/{reg} -> {e}")
                continue

            for u in urls:
                try:
                    item = fetch_and_parse(u)
                    if not item:
                        continue

                    # --- [NEW] ì œëª© ì„ë² ë”© ì¶”ê°€ (KO-BERT 768ì°¨ì›) ---
                    try:
                        emb = embed_title(item.get("title") or "")
                        if emb:
                            item["title_embedding"] = {
                                "model": EMBED_MODEL_NAME,
                                "vector": emb,     # 768 floats
                                "dim": len(emb),   # 768
                                "normalized": True
                            }
                    except Exception as e:
                        print("[EMB-ERR] title embedding:", e)
                    # --- [NEW end] ---

                    if save_item_if_new(item, seen_ids):
                        n_imgs = 0
                        if isinstance(item.get("body"), list):
                            n_imgs = sum(1 for b in item["body"] if b.get("type") == "image")
                        elif isinstance(item.get("body"), str):
                            n_imgs = len(IMG_URL_RE.findall(item["body"]))
                        print(f"[LIST][{yyyymmdd_from_iso(item.get('published_at')) or 'unknown'}] {item.get('title')} (sid1={sid1}) | imgs={n_imgs} | sections={item.get('section')}")
                        rand_sleep(DETAIL_DELAY)
                except KeyboardInterrupt:
                    raise
                except Exception as e:
                    print("[ERR]", u, e)

    print("âœ” ë¦¬ìŠ¤íŠ¸ ì „ìš© í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"  - ê³ ìœ  ê¸°ì‚¬ ìˆ˜: {len(seen_ids):,}ê°œ (ì¤‘ë³µ ì œê±°ë¨)")
    print("  - ì €ì¥ ìœ„ì¹˜: output/ (ë°œí–‰ì¼ë³„ JSONL íŒŒì¼)")
    print(f"  - ë³¸ë¬¸ ëª¨ë“œ: {INLINE_IMAGES_MODE} ({'ë¸”ë¡ ë°°ì—´' if INLINE_IMAGES_MODE == 'blocks' else 'ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´'})")
    if DOWNLOAD_IMAGES:
        print(f"  - ì´ë¯¸ì§€ ì €ì¥ ìœ„ì¹˜: {IMAGES_DIR}/ (ë‚ ì§œë³„ ë””ë ‰í† ë¦¬)")
