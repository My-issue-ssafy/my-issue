# Content-Based Filtering (CBF) ì¶”ì²œ ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ
# ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„°ì™€ ë‰´ìŠ¤ ì„ë² ë”©ì„ í™œìš©í•œ ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ

import os
import pickle
import json
import numpy as np
import pandas as pd
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
from tqdm import tqdm

from app.config import PROJECT_ID, DEFAULT_DATASET, LOOKBACK_DAYS
from app.core.analytics.bq import get_client, get_latest_date
from app.core.ml.train_cf import today_kst_str, already_trained_today as cf_already_trained_today
from app.db.models.news import News
from app.utils.config import settings

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
MODELS_DIR = Path(__file__).resolve().parent.parent.parent / "models"
CBF_MODEL_PATH = MODELS_DIR / "cbf_model.pkl"
CBF_METADATA_PATH = MODELS_DIR / "cbf_metadata.json"

# ë§ˆì§€ë§‰ CBF í•™ìŠµ ë‚ ì§œë¥¼ ê¸°ë¡í•˜ëŠ” íŒŒì¼ ê²½ë¡œ
CBF_STATE_PATH = os.path.join(os.path.dirname(__file__), "..", "..", "..", "cbf_last_trained.txt")

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
engine = create_engine(settings.DATABASE_URL, pool_pre_ping=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def already_trained_cbf_today() -> bool:
    """ì˜¤ëŠ˜ ì´ë¯¸ CBF ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí–ˆëŠ”ì§€ í™•ì¸"""
    if not os.path.exists(CBF_STATE_PATH):
        return False
    with open(CBF_STATE_PATH, "r", encoding="utf-8") as f:
        last = f.read().strip()
    return last == today_kst_str()

def mark_cbf_trained_today():
    """ì˜¤ëŠ˜ CBF ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí–ˆë‹¤ëŠ” í‘œì‹œë¥¼ íŒŒì¼ì— ê¸°ë¡"""
    os.makedirs(os.path.dirname(CBF_STATE_PATH), exist_ok=True)
    with open(CBF_STATE_PATH, "w", encoding="utf-8") as f:
        f.write(today_kst_str())

def load_news_embeddings() -> Tuple[pd.DataFrame, np.ndarray]:
    """
    PostgreSQLì—ì„œ ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„°ì™€ ì„ë² ë”© ë²¡í„°ë¥¼ ë¡œë“œ
    
    Returns:
        news_df: ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° DataFrame
        embeddings_matrix: ë‰´ìŠ¤ ì„ë² ë”© í–‰ë ¬ (n_news x embedding_dim)
    """
    from datetime import datetime, timedelta
    from zoneinfo import ZoneInfo
    
    db = SessionLocal()
    try:
        # ì„±ëŠ¥ ìµœì í™”: ì„ë² ë”©ì´ ìˆëŠ” ìµœì‹  ë‰´ìŠ¤ 500ê°œë§Œ ë¹ ë¥´ê²Œ ì¡°íšŒ
        print(f"[INFO] Loading latest 500 news with embeddings (optimized for performance)")
        
        # ì„ë² ë”©ì´ ìˆëŠ” ìµœì‹  ë‰´ìŠ¤ë§Œ ì œí•œëœ ìˆ˜ë¡œ ì¡°íšŒ (ì„±ëŠ¥ ê°œì„ )
        news_query = db.query(News).filter(
            News.embedding.isnot(None)
        ).order_by(News.created_at.desc()).limit(500).all()
        
        if not news_query:
            print("[ERROR] No news with embeddings found at all")
            return pd.DataFrame(), np.array([])
        
        # ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° DataFrame ìƒì„±
        print(f"[INFO] Processing {len(news_query)} news embeddings...")
        news_data = []
        embeddings = []
        
        for news in tqdm(news_query, desc="Processing news embeddings"):
            news_data.append({
                'news_id': news.id,
                'title': news.title,
                'category': news.category,
                'news_paper': news.news_paper,
                'created_at': news.created_at,
                'views': news.views
            })
            embeddings.append(news.embedding)
        
        news_df = pd.DataFrame(news_data)
        embeddings_matrix = np.array(embeddings)
        
        print(f"[INFO] Loaded {len(news_df)} news with embeddings")
        print(f"[INFO] Embedding dimension: {embeddings_matrix.shape[1] if len(embeddings_matrix) > 0 else 0}")
        print(f"[INFO] Date range: {news_df['created_at'].min()} ~ {news_df['created_at'].max()}")
        
        return news_df, embeddings_matrix
        
    except Exception as e:
        print(f"[ERROR] Failed to load news embeddings: {e}")
        return pd.DataFrame(), np.array([])
    finally:
        db.close()

def create_user_profiles(interactions_df: pd.DataFrame, news_df: pd.DataFrame, 
                        embeddings_matrix: np.ndarray) -> Tuple[Dict, Dict]:
    """
    ì‚¬ìš©ìë³„ í”„ë¡œí•„ ë²¡í„° ìƒì„± (ìƒí˜¸ì‘ìš©í•œ ë‰´ìŠ¤ë“¤ì˜ ê°€ì¤‘ í‰ê·  ì„ë² ë”©)
    
    Args:
        interactions_df: ì‚¬ìš©ì-ë‰´ìŠ¤ ìƒí˜¸ì‘ìš© ë°ì´í„°
        news_df: ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°
        embeddings_matrix: ë‰´ìŠ¤ ì„ë² ë”© í–‰ë ¬
        
    Returns:
        user_profiles: {user_id: profile_vector}
        user_stats: {user_id: {'total_interactions': int, 'avg_strength': float}}
    """
    if embeddings_matrix.size == 0:
        return {}, {}
    
    # ë‰´ìŠ¤ IDë¥¼ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘
    news_id_to_idx = {news_id: idx for idx, news_id in enumerate(news_df['news_id'].values)}
    
    user_profiles = {}
    user_stats = {}
    
    unique_users = interactions_df['user_id'].unique()
    print(f"[INFO] Creating user profiles for {len(unique_users)} users...")
    
    for user_id in tqdm(unique_users, desc="Creating user profiles"):
        user_interactions = interactions_df[interactions_df['user_id'] == user_id]
        
        # í•´ë‹¹ ì‚¬ìš©ìê°€ ìƒí˜¸ì‘ìš©í•œ ë‰´ìŠ¤ë“¤ì˜ ì„ë² ë”©ê³¼ ê°•ë„
        user_embeddings = []
        user_weights = []
        
        for _, row in user_interactions.iterrows():
            news_id = row['news_id']
            strength = row['strength']
            
            if news_id in news_id_to_idx:
                idx = news_id_to_idx[news_id]
                user_embeddings.append(embeddings_matrix[idx])
                user_weights.append(strength)
        
        if user_embeddings:
            user_embeddings = np.array(user_embeddings)
            user_weights = np.array(user_weights)
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±
            profile_vector = np.average(user_embeddings, axis=0, weights=user_weights)
            
            # ì •ê·œí™”
            profile_vector = normalize([profile_vector])[0]
            
            user_profiles[user_id] = profile_vector
            user_stats[user_id] = {
                'total_interactions': len(user_interactions),
                'avg_strength': user_interactions['strength'].mean(),
                'interacted_news_count': len(user_embeddings)
            }
    
    print(f"[INFO] Created profiles for {len(user_profiles)} users")
    return user_profiles, user_stats

def generate_recommendations(user_profiles: Dict, news_df: pd.DataFrame, 
                           embeddings_matrix: np.ndarray, top_k: int = 50) -> Dict:
    """
    ì‚¬ìš©ìë³„ CBF ì¶”ì²œ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)
    
    Args:
        user_profiles: ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„°ë“¤
        news_df: ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°
        embeddings_matrix: ë‰´ìŠ¤ ì„ë² ë”© í–‰ë ¬
        top_k: ì‚¬ìš©ìë³„ ì¶”ì²œí•  ë‰´ìŠ¤ ìˆ˜
        
    Returns:
        recommendations: {user_id: [{'news_id': int, 'score': float, 'title': str}, ...]}
    """
    if embeddings_matrix.size == 0:
        return {}
    
    recommendations = {}
    
    # ë‰´ìŠ¤ ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™”)
    print(f"[INFO] Normalizing embeddings matrix ({embeddings_matrix.shape})...")
    normalized_embeddings = normalize(embeddings_matrix)
    
    print(f"[INFO] Generating recommendations for {len(user_profiles)} users...")
    for user_id, profile_vector in tqdm(user_profiles.items(), desc="Generating recommendations"):
        # ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ëª¨ë“  ë‰´ìŠ¤ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity([profile_vector], normalized_embeddings)[0]
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_indices = np.argsort(-similarities)[:top_k]
        
        user_recommendations = []
        for idx in similar_indices:
            news_id = news_df.iloc[idx]['news_id']
            score = similarities[idx]
            title = news_df.iloc[idx]['title']
            category = news_df.iloc[idx]['category']
            
            user_recommendations.append({
                'news_id': int(news_id),
                'score': float(score),
                'title': title,
                'category': category
            })
        
        recommendations[user_id] = user_recommendations
    
    print(f"[INFO] Generated recommendations for {len(recommendations)} users")
    print(f"[INFO] Average recommendations per user: {top_k}")
    
    return recommendations

def save_cbf_model(recommendations: Dict, user_profiles: Dict, user_stats: Dict, 
                   news_count: int, embedding_dim: int):
    """
    CBF ëª¨ë¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        recommendations: ì‚¬ìš©ìë³„ ì¶”ì²œ ê²°ê³¼
        user_profiles: ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„°ë“¤
        user_stats: ì‚¬ìš©ìë³„ í†µê³„ ì •ë³´
        news_count: ì „ì²´ ë‰´ìŠ¤ ìˆ˜
        embedding_dim: ì„ë² ë”© ì°¨ì›
    """
    # models ë””ë ‰í† ë¦¬ ìƒì„±
    MODELS_DIR.mkdir(exist_ok=True)
    
    # CBF ëª¨ë¸ ë°ì´í„° íŒ¨í‚¤ì§•
    cbf_model_data = {
        'recommendations': recommendations,
        'user_profiles': user_profiles,
        'user_stats': user_stats,
        'trained_at': datetime.now(timezone.utc),
        'train_date': today_kst_str(),
        'model_params': {
            'total_users': len(user_profiles),
            'total_news': news_count,
            'embedding_dim': embedding_dim,
            'top_k_recommendations': 50,
            'similarity_metric': 'cosine'
        }
    }
    
    # ëª¨ë¸ì„ pickle íŒŒì¼ë¡œ ì €ì¥
    with open(CBF_MODEL_PATH, 'wb') as f:
        pickle.dump(cbf_model_data, f)
    
    # ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë””ë²„ê¹… ë° í™•ì¸ìš©)
    metadata = {
        'trained_at': cbf_model_data['trained_at'].isoformat(),
        'train_date': cbf_model_data['train_date'],
        'total_users': int(len(user_profiles)),
        'total_news': int(news_count),
        'embedding_dim': int(embedding_dim),
        'model_file': str(CBF_MODEL_PATH.name),
        'sample_users': [int(uid) for uid in list(recommendations.keys())[:5]] if recommendations else []
    }
    
    with open(CBF_METADATA_PATH, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"[INFO] CBF model saved to {CBF_MODEL_PATH}")
    print(f"[INFO] CBF metadata saved to {CBF_METADATA_PATH}")
    print(f"[INFO] Users: {len(user_profiles)}, News: {news_count}")

def load_interactions_from_csv() -> pd.DataFrame:
    """
    data/ ë””ë ‰í† ë¦¬ì˜ interactions_*.csv íŒŒì¼ë“¤ì„ ëª¨ë‘ ì½ì–´ì„œ í†µí•©ëœ ìƒí˜¸ì‘ìš© ë°ì´í„° ë°˜í™˜
    
    Returns:
        interactions_df: ì‚¬ìš©ì-ë‰´ìŠ¤ ìƒí˜¸ì‘ìš© DataFrame (user_id, news_id, strength)
    """
    import glob
    
    # data ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  interactions_*.csv íŒŒì¼ ì°¾ê¸°
    data_dir = Path(__file__).resolve().parent.parent.parent.parent / "data"
    csv_pattern = str(data_dir / "interactions_*.csv")
    csv_files = glob.glob(csv_pattern)
    
    # 14ì¼ì¹˜ â†’ 1ì¼ì¹˜ë¡œ ë³€ê²½: ê°€ì¥ ìµœì‹  íŒŒì¼ë§Œ ì‚¬ìš©
    if csv_files:
        # íŒŒì¼ëª…ìœ¼ë¡œ ì •ë ¬í•´ì„œ ê°€ì¥ ìµœì‹  íŒŒì¼ í•˜ë‚˜ë§Œ ì„ íƒ (interactions_YYYYMMDD.csv í˜•ì‹)
        csv_files = [sorted(csv_files)[-1]]
        print(f"[INFO] Using only the latest CSV file (1 day) instead of all files")
    
    if not csv_files:
        print(f"[ERROR] No interactions CSV files found in {data_dir}")
        return pd.DataFrame()
    
    print(f"[INFO] Found {len(csv_files)} interaction CSV files:")
    for f in sorted(csv_files):
        file_size = Path(f).stat().st_size / (1024*1024)  # MB
        print(f"  - {Path(f).name} ({file_size:.1f}MB)")
    
    # ëª¨ë“  CSV íŒŒì¼ì„ ì½ì–´ì„œ í†µí•©
    dfs = []
    total_rows = 0
    
    print(f"[INFO] Loading CSV files...")
    for csv_file in tqdm(sorted(csv_files), desc="Loading CSV files"):
        try:
            print(f"  Loading {Path(csv_file).name}... ", end="")
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            # ì»¬ëŸ¼ëª… ì •ë¦¬ (BOM ì œê±°)
            df.columns = df.columns.str.strip()
            
            if not df.empty and all(col in df.columns for col in ['user_id', 'news_id', 'strength']):
                dfs.append(df)
                total_rows += len(df)
                print(f"OK ({len(df):,} interactions)")
            else:
                print(f"ERROR - Missing required columns")
                
        except Exception as e:
            print(f"ERROR - {e}")
    
    if not dfs:
        print("[ERROR] No valid interaction data loaded from CSV files")
        return pd.DataFrame()
    
    # ëª¨ë“  ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ì¤‘ë³µ ì œê±°
    print(f"[INFO] Combining and deduplicating data...")
    combined_df = pd.concat(dfs, ignore_index=True)
    
    # ê°™ì€ user_id, news_id ì¡°í•©ì´ ìˆìœ¼ë©´ strength ê°’ì„ í•©ê³„ (ìƒí˜¸ì‘ìš© ëˆ„ì )
    print(f"[INFO] Grouping by user_id and news_id...")
    combined_df = combined_df.groupby(['user_id', 'news_id'], as_index=False)['strength'].sum()
    
    print(f"[INFO] Total interactions loaded: {total_rows:,}")
    print(f"[INFO] After deduplication: {len(combined_df):,}")
    print(f"[INFO] Unique users: {combined_df['user_id'].nunique():,}")
    print(f"[INFO] Unique news: {combined_df['news_id'].nunique():,}")
    print(f"[INFO] Strength range: {combined_df['strength'].min():.2f} ~ {combined_df['strength'].max():.2f}")
    
    return combined_df

def test_cbf_recommendations(recommendations: Dict, user_stats: Dict, interactions_df: pd.DataFrame, num_test_users: int = 3):
    """
    CBF ì¶”ì²œ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì¶œë ¥ (train_cf ìŠ¤íƒ€ì¼)
    
    Args:
        recommendations: ì‚¬ìš©ìë³„ ì¶”ì²œ ê²°ê³¼
        user_stats: ì‚¬ìš©ìë³„ í†µê³„ ì •ë³´
        interactions_df: ì›ë³¸ ìƒí˜¸ì‘ìš© ë°ì´í„°
        num_test_users: í…ŒìŠ¤íŠ¸í•  ì‚¬ìš©ì ìˆ˜
    """
    if not recommendations:
        print("[WARNING] No recommendations to test")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ CBF ì¶”ì²œ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ({num_test_users}ëª…ì˜ ìƒ˜í”Œ ì‚¬ìš©ì)")
    print(f"{'='*60}")
    
    # ëœë¤í•˜ê²Œ ì‚¬ìš©ì ì„ íƒ
    import random
    all_users = list(recommendations.keys())
    sample_users = random.sample(all_users, min(num_test_users, len(all_users)))
    
    for idx, user_id in enumerate(sample_users, 1):
        user_recs = recommendations[user_id]
        stats = user_stats.get(user_id, {})
        
        print(f"\nğŸ“Š [{idx}] User ID: {user_id}")
        print(f"   ğŸ’¬ ì´ ìƒí˜¸ì‘ìš©: {stats.get('total_interactions', 0)}íšŒ")
        print(f"   ğŸ¯ ìƒí˜¸ì‘ìš© ë‰´ìŠ¤ ìˆ˜: {stats.get('interacted_news_count', 0)}ê°œ")
        print(f"   ğŸ“ˆ í‰ê·  ìƒí˜¸ì‘ìš© ê°•ë„: {stats.get('avg_strength', 0):.3f}")
        
        # ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ìƒí˜¸ì‘ìš©í•œ ë‰´ìŠ¤ë“¤ í‘œì‹œ
        user_interactions = interactions_df[interactions_df['user_id'] == user_id]
        if not user_interactions.empty:
            top_interactions = user_interactions.nlargest(3, 'strength')
            print(f"\n   ğŸ“– ìµœê³  ìƒí˜¸ì‘ìš© ë‰´ìŠ¤:")
            for i, (_, row) in enumerate(top_interactions.iterrows(), 1):
                print(f"      {i}. News ID {row['news_id']} (ê°•ë„: {row['strength']:.2f})")
        
        # CBF ì¶”ì²œ ê²°ê³¼
        print(f"\n   ğŸ¤– CBF ì¶”ì²œ ë‰´ìŠ¤ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜):")
        for i, rec in enumerate(user_recs[:5], 1):  # ìƒìœ„ 5ê°œ
            title_short = rec['title'][:45] + "..." if len(rec['title']) > 45 else rec['title']
            category_display = f"[{rec['category']}]" if rec['category'] else "[ê¸°íƒ€]"
            print(f"      {i}. {category_display} {title_short}")
            print(f"          â†’ ìœ ì‚¬ë„: {rec['score']:.4f} | News ID: {rec['news_id']}")
        
        print(f"   {'â”€'*50}")
    
    # ì „ì²´ í†µê³„
    total_users = len(recommendations)
    avg_recs_per_user = sum(len(recs) for recs in recommendations.values()) / total_users
    
    print(f"\nğŸ“ˆ CBF ëª¨ë¸ ì „ì²´ í†µê³„:")
    print(f"   â€¢ í”„ë¡œí•„ì´ ìƒì„±ëœ ì‚¬ìš©ì: {total_users:,}ëª…")
    print(f"   â€¢ ì‚¬ìš©ìë‹¹ í‰ê·  ì¶”ì²œ ìˆ˜: {avg_recs_per_user:.1f}ê°œ")
    print(f"   â€¢ ìœ ì‚¬ë„ ë²”ìœ„: {min([min([r['score'] for r in recs]) for recs in recommendations.values()]):.4f} ~ {max([max([r['score'] for r in recs]) for recs in recommendations.values()]):.4f}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œ ë¶„í¬ 
    all_categories = {}
    for recs in recommendations.values():
        for rec in recs[:10]:  # ìƒìœ„ 10ê°œë§Œ ë¶„ì„
            cat = rec['category'] or 'ê¸°íƒ€'
            all_categories[cat] = all_categories.get(cat, 0) + 1
    
    print(f"\nğŸ“Š ì¶”ì²œ ì¹´í…Œê³ ë¦¬ ë¶„í¬ (ìƒìœ„ 5ê°œ):")
    top_categories = sorted(all_categories.items(), key=lambda x: x[1], reverse=True)[:5]
    for cat, count in top_categories:
        percentage = (count / sum(all_categories.values())) * 100
        print(f"   â€¢ {cat}: {count:,}íšŒ ({percentage:.1f}%)")
    
    print(f"\nâœ… CBF ì¶”ì²œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

def main():
    """
    CBF ì¶”ì²œ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ë©”ì¸ í•¨ìˆ˜
    
    1. ì˜¤ëŠ˜ ì´ë¯¸ CBF í•™ìŠµì„ ì™„ë£Œí–ˆëŠ”ì§€ í™•ì¸
    2. BigQueryì—ì„œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„° ë¡œë“œ
    3. PostgreSQLì—ì„œ ë‰´ìŠ¤ ì„ë² ë”© ë°ì´í„° ë¡œë“œ
    4. ì‚¬ìš©ìë³„ í”„ë¡œí•„ ë²¡í„° ìƒì„±
    5. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ CBF ì¶”ì²œ ìƒì„±
    6. ëª¨ë¸ ê²°ê³¼ë¥¼ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥
    """
    print(f"[START] CBF ëª¨ë¸ í•™ìŠµ ì‹œì‘ - {datetime.now()}")
    
    # 1. ì´ë¯¸ ì˜¤ëŠ˜ CBF í•™ìŠµì„ ì™„ë£Œí–ˆëŠ”ì§€ í™•ì¸
    if already_trained_cbf_today():
        print("[SKIP] CBF model already trained today.")
        return
    
    # 2. CSV íŒŒì¼ì—ì„œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„° ë¡œë“œ
    try:
        interactions_df = load_interactions_from_csv()
        print(f"[INFO] Loaded {len(interactions_df)} user interactions from CSV files")
        
        if interactions_df.empty:
            print("[SKIP] No interaction data available for CBF training")
            return
            
    except Exception as e:
        print(f"[ERROR] Failed to load interaction data from CSV: {e}")
        return
    
    # 3. PostgreSQLì—ì„œ ë‰´ìŠ¤ ì„ë² ë”© ë°ì´í„° ë¡œë“œ
    news_df, embeddings_matrix = load_news_embeddings()
    
    if news_df.empty or embeddings_matrix.size == 0:
        print("[SKIP] No news embeddings available for CBF training")
        return
    
    # 4. ì‚¬ìš©ìë³„ í”„ë¡œí•„ ë²¡í„° ìƒì„±
    user_profiles, user_stats = create_user_profiles(interactions_df, news_df, embeddings_matrix)
    
    if not user_profiles:
        print("[SKIP] No user profiles created for CBF training")
        return
    
    # 5. CBF ì¶”ì²œ ìƒì„±
    recommendations = generate_recommendations(user_profiles, news_df, embeddings_matrix, top_k=50)
    
    if not recommendations:
        print("[SKIP] No recommendations generated")
        return
    
    # 6. CBF ëª¨ë¸ ì €ì¥
    save_cbf_model(recommendations, user_profiles, user_stats, 
                   len(news_df), embeddings_matrix.shape[1])
    
    # 7. í…ŒìŠ¤íŠ¸ìš© ì¶”ì²œ ê²°ê³¼ ì¶œë ¥ (train_cf ìŠ¤íƒ€ì¼)
    test_cbf_recommendations(recommendations, user_stats, interactions_df, num_test_users=3)
    
    # 8. ì˜¤ëŠ˜ CBF í•™ìŠµ ì™„ë£Œ ìƒíƒœ ê¸°ë¡
    mark_cbf_trained_today()
    
    print(f"[SUCCESS] CBF ëª¨ë¸ í•™ìŠµ ì™„ë£Œ - {datetime.now()}")

if __name__ == "__main__":
    main()