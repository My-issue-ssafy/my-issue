# Content-Based Filtering (CBF) ì¶”ì²œ ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ
# ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„°ì™€ ë‰´ìŠ¤ ì„ë² ë”©ì„ í™œìš©í•œ ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ

import os
import pickle
import json
import numpy as np
import pandas as pd
import psutil
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
from tqdm import tqdm

from app.config import PROJECT_ID, DEFAULT_DATASET, LOOKBACK_DAYS
from app.core.analytics.bq import get_client, get_latest_date
from app.core.ml.train_cf import today_kst_str, already_trained_today as cf_already_trained_today
from app.db.models.news import News
from app.utils.config import settings

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
MODELS_DIR = Path(__file__).resolve().parent.parent.parent / "models"
CBF_MODEL_PATH = MODELS_DIR / "cbf_model.pkl"
CBF_METADATA_PATH = MODELS_DIR / "cbf_metadata.json"

# ë§ˆì§€ë§‰ CBF í•™ìŠµ ë‚ ì§œë¥¼ ê¸°ë¡í•˜ëŠ” íŒŒì¼ ê²½ë¡œ
CBF_STATE_PATH = os.path.join(os.path.dirname(__file__), "..", "..", "..", "cbf_last_trained.txt")

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
engine = create_engine(settings.DATABASE_URL, pool_pre_ping=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def already_trained_cbf_today() -> bool:
    """ì˜¤ëŠ˜ ì´ë¯¸ CBF ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí–ˆëŠ”ì§€ í™•ì¸"""
    if not os.path.exists(CBF_STATE_PATH):
        return False
    with open(CBF_STATE_PATH, "r", encoding="utf-8") as f:
        last = f.read().strip()
    return last == today_kst_str()

def mark_cbf_trained_today():
    """ì˜¤ëŠ˜ CBF ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí–ˆë‹¤ëŠ” í‘œì‹œë¥¼ íŒŒì¼ì— ê¸°ë¡"""
    os.makedirs(os.path.dirname(CBF_STATE_PATH), exist_ok=True)
    with open(CBF_STATE_PATH, "w", encoding="utf-8") as f:
        f.write(today_kst_str())

def load_news_embeddings() -> Tuple[pd.DataFrame, np.ndarray]:
    """
    PostgreSQLì—ì„œ ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„°ì™€ ì„ë² ë”© ë²¡í„°ë¥¼ ë¡œë“œ
    
    Returns:
        news_df: ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° DataFrame
        embeddings_matrix: ë‰´ìŠ¤ ì„ë² ë”© í–‰ë ¬ (n_news x embedding_dim)
    """
    from datetime import datetime, timedelta
    from zoneinfo import ZoneInfo
    
    db = SessionLocal()
    try:
        # 24ì‹œê°„ ì´ë‚´ì˜ ì„ë² ë”©ì´ ìˆëŠ” ë‰´ìŠ¤ ì¡°íšŒ
        kst = ZoneInfo("Asia/Seoul")
        now_kst = datetime.now(kst)
        yesterday_kst = now_kst - timedelta(hours=24)

        print(f"[INFO] ìµœê·¼ 24ì‹œê°„ ì„ë² ë”© ë‰´ìŠ¤ ë¡œë“œ ì¤‘ ({yesterday_kst.strftime('%Y-%m-%d %H:%M')} ~ {now_kst.strftime('%Y-%m-%d %H:%M')})")

        # 24ì‹œê°„ ì´ë‚´ + ì„ë² ë”©ì´ ìˆëŠ” ë‰´ìŠ¤ ì¡°íšŒ
        news_query = db.query(News).filter(
            News.embedding.isnot(None),
            News.created_at >= yesterday_kst
        ).order_by(News.created_at.desc()).all()
        
        if not news_query:
            print("[ERROR] ì„ë² ë”©ì´ ìˆëŠ” ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame(), np.array([])
        
        # ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° DataFrame ìƒì„± (ìµœì í™”ëœ ë°©ì‹)
        print(f"[INFO] {len(news_query)}ê°œ ë‰´ìŠ¤ ì„ë² ë”© ì²˜ë¦¬ ì¤‘ (ìµœì í™”ë¨)...")

        # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ í•œ ë²ˆì— ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ )
        news_data = [
            {
                'news_id': news.id,
                'title': news.title,
                'category': news.category,
                'news_paper': news.news_paper,
                'created_at': news.created_at,
                'views': news.views
            }
            for news in news_query
        ]

        # ì„ë² ë”©ë„ í•œ ë²ˆì— ì¶”ì¶œ (tqdm ì œê±°ë¡œ ì˜¤ë²„í—¤ë“œ ê°ì†Œ)
        embeddings = [news.embedding for news in news_query]

        news_df = pd.DataFrame(news_data)
        embeddings_matrix = np.array(embeddings)
        
        print(f"[INFO] {len(news_df)}ê°œ ì„ë² ë”© ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        print(f"[INFO] ì„ë² ë”© ì°¨ì›: {embeddings_matrix.shape[1] if len(embeddings_matrix) > 0 else 0}")
        print(f"[INFO] ë‚ ì§œ ë²”ìœ„: {news_df['created_at'].min()} ~ {news_df['created_at'].max()}")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        memory_mb = embeddings_matrix.nbytes / (1024 * 1024)
        print(f"[INFO] ì„ë² ë”© í–‰ë ¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")
        process = psutil.Process()
        total_memory_mb = process.memory_info().rss / (1024 * 1024)
        print(f"[INFO] í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ì´ ë©”ëª¨ë¦¬: {total_memory_mb:.1f}MB")
        
        return news_df, embeddings_matrix
        
    except Exception as e:
        print(f"[ERROR] ë‰´ìŠ¤ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame(), np.array([])
    finally:
        db.close()

def create_user_profiles(interactions_df: pd.DataFrame, news_df: pd.DataFrame,
                        embeddings_matrix: np.ndarray) -> Tuple[Dict, Dict]:
    """
    ì‚¬ìš©ìë³„ í”„ë¡œí•„ ë²¡í„° ìƒì„± (ìµœì í™”ëœ ê°€ì¤‘ í‰ê·  ì„ë² ë”©)

    Args:
        interactions_df: ì‚¬ìš©ì-ë‰´ìŠ¤ ìƒí˜¸ì‘ìš© ë°ì´í„°
        news_df: ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°
        embeddings_matrix: ë‰´ìŠ¤ ì„ë² ë”© í–‰ë ¬

    Returns:
        user_profiles: {user_id: profile_vector}
        user_stats: {user_id: {'total_interactions': int, 'avg_strength': float}}
    """
    if embeddings_matrix.size == 0:
        return {}, {}

    # ë‰´ìŠ¤ IDë¥¼ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘
    news_id_to_idx = {news_id: idx for idx, news_id in enumerate(news_df['news_id'].values)}

    # ìƒí˜¸ì‘ìš© ë°ì´í„°ì— ì„ë² ë”© ì¸ë±ìŠ¤ ì¶”ê°€ (ë²¡í„°í™”ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬)
    print(f"[INFO] ë²¡í„°í™”ë¥¼ ìœ„í•œ ìƒí˜¸ì‘ìš© ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    interactions_df_copy = interactions_df.copy()
    interactions_df_copy['embedding_idx'] = interactions_df_copy['news_id'].map(news_id_to_idx)

    # ì„ë² ë”©ì´ ì—†ëŠ” ë‰´ìŠ¤ ì œê±°
    valid_interactions = interactions_df_copy.dropna(subset=['embedding_idx'])
    valid_interactions['embedding_idx'] = valid_interactions['embedding_idx'].astype(int)

    user_profiles = {}
    user_stats = {}

    unique_users = valid_interactions['user_id'].unique()
    print(f"[INFO] {len(unique_users)}ëª… ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì¤‘ (ìµœì í™”ë¨)...")

    # ì‚¬ìš©ìë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í•œë²ˆì— ì²˜ë¦¬
    grouped = valid_interactions.groupby('user_id')

    for user_id, user_data in tqdm(grouped, desc="Creating user profiles"):
        # í•´ë‹¹ ì‚¬ìš©ìì˜ ëª¨ë“  ì„ë² ë”©ê³¼ ê°€ì¤‘ì¹˜ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ
        embedding_indices = user_data['embedding_idx'].values
        weights = user_data['strength'].values

        if len(embedding_indices) > 0:
            # ë²¡í„°í™”ëœ ì„ë² ë”© ì¶”ì¶œ (fancy indexing)
            user_embeddings = embeddings_matrix[embedding_indices]

            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±
            profile_vector = np.average(user_embeddings, axis=0, weights=weights)

            # ì •ê·œí™”
            profile_vector = normalize([profile_vector])[0]

            user_profiles[user_id] = profile_vector
            user_stats[user_id] = {
                'total_interactions': len(user_data),
                'avg_strength': user_data['strength'].mean(),
                'interacted_news_count': len(embedding_indices)
            }

    print(f"[INFO] {len(user_profiles)}ëª… ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì™„ë£Œ")
    return user_profiles, user_stats

def generate_recommendations(user_profiles: Dict, news_df: pd.DataFrame,
                           embeddings_matrix: np.ndarray, top_k: int = 50) -> Dict:
    """
    ì‚¬ìš©ìë³„ CBF ì¶”ì²œ ìƒì„± (ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ìµœì í™”ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)

    Args:
        user_profiles: ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„°ë“¤
        news_df: ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°
        embeddings_matrix: ë‰´ìŠ¤ ì„ë² ë”© í–‰ë ¬
        top_k: ì‚¬ìš©ìë³„ ì¶”ì²œí•  ë‰´ìŠ¤ ìˆ˜

    Returns:
        recommendations: {user_id: [{'news_id': int, 'score': float, 'title': str}, ...]}
    """
    if embeddings_matrix.size == 0:
        return {}

    # ë‰´ìŠ¤ ì„ë² ë”© ì •ê·œí™” (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    print(f"[INFO] ì„ë² ë”© í–‰ë ¬ ì •ê·œí™” ì¤‘ ({embeddings_matrix.shape})...")
    normalized_embeddings = normalize(embeddings_matrix)

    # ì‚¬ìš©ì í”„ë¡œí•„ì„ í–‰ë ¬ë¡œ ë³€í™˜ (ë²¡í„°í™” ì—°ì‚°ì„ ìœ„í•´)
    user_ids = list(user_profiles.keys())
    user_profiles_matrix = np.array(list(user_profiles.values()))

    print(f"[INFO] {len(user_profiles)}ëª… ì‚¬ìš©ì ì¶”ì²œ ìƒì„± ì¤‘ (ë²¡í„°í™” ì—°ì‚°)...")

    # ëª¨ë“  ì‚¬ìš©ì-ë‰´ìŠ¤ ê°„ ìœ ì‚¬ë„ë¥¼ í•œ ë²ˆì— ê³„ì‚° (ë²¡í„°í™” ì—°ì‚°)
    all_similarities = cosine_similarity(user_profiles_matrix, normalized_embeddings)

    # ê° ì‚¬ìš©ìë³„ë¡œ ì¶”ì²œ ê²°ê³¼ ìƒì„±
    recommendations = {}
    for i, user_id in enumerate(tqdm(user_ids, desc="Processing user recommendations")):
        similarities = all_similarities[i]

        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_indices = np.argsort(-similarities)[:top_k]

        user_recommendations = []
        for idx in similar_indices:
            news_id = news_df.iloc[idx]['news_id']
            score = similarities[idx]
            title = news_df.iloc[idx]['title']
            category = news_df.iloc[idx]['category']

            user_recommendations.append({
                'news_id': int(news_id),
                'score': float(score),
                'title': title,
                'category': category
            })

        recommendations[user_id] = user_recommendations

    print(f"[INFO] {len(recommendations)}ëª… ì‚¬ìš©ì ì¶”ì²œ ìƒì„± ì™„ë£Œ")
    print(f"[INFO] ì‚¬ìš©ìë‹¹ í‰ê·  ì¶”ì²œ ìˆ˜: {top_k}ê°œ")

    return recommendations

def save_cbf_model(recommendations: Dict, user_profiles: Dict, user_stats: Dict, 
                   news_count: int, embedding_dim: int):
    """
    CBF ëª¨ë¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        recommendations: ì‚¬ìš©ìë³„ ì¶”ì²œ ê²°ê³¼
        user_profiles: ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„°ë“¤
        user_stats: ì‚¬ìš©ìë³„ í†µê³„ ì •ë³´
        news_count: ì „ì²´ ë‰´ìŠ¤ ìˆ˜
        embedding_dim: ì„ë² ë”© ì°¨ì›
    """
    # models ë””ë ‰í† ë¦¬ ìƒì„±
    MODELS_DIR.mkdir(exist_ok=True)
    
    # CBF ëª¨ë¸ ë°ì´í„° íŒ¨í‚¤ì§•
    cbf_model_data = {
        'recommendations': recommendations,
        'user_profiles': user_profiles,
        'user_stats': user_stats,
        'trained_at': datetime.now(timezone.utc),
        'train_date': today_kst_str(),
        'model_params': {
            'total_users': len(user_profiles),
            'total_news': news_count,
            'embedding_dim': embedding_dim,
            'top_k_recommendations': 50,
            'similarity_metric': 'cosine'
        }
    }
    
    # ëª¨ë¸ì„ pickle íŒŒì¼ë¡œ ì €ì¥
    with open(CBF_MODEL_PATH, 'wb') as f:
        pickle.dump(cbf_model_data, f)
    
    # ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë””ë²„ê¹… ë° í™•ì¸ìš©)
    metadata = {
        'trained_at': cbf_model_data['trained_at'].isoformat(),
        'train_date': cbf_model_data['train_date'],
        'total_users': int(len(user_profiles)),
        'total_news': int(news_count),
        'embedding_dim': int(embedding_dim),
        'model_file': str(CBF_MODEL_PATH.name),
        'sample_users': [int(uid) for uid in list(recommendations.keys())[:5]] if recommendations else []
    }
    
    with open(CBF_METADATA_PATH, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"[INFO] CBF ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {CBF_MODEL_PATH}")
    print(f"[INFO] CBF ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {CBF_METADATA_PATH}")
    print(f"[INFO] ì‚¬ìš©ì: {len(user_profiles)}ëª…, ë‰´ìŠ¤: {news_count}ê°œ")

def load_interactions_from_csv() -> pd.DataFrame:
    """
    data/ ë””ë ‰í† ë¦¬ì˜ interactions_*.csv íŒŒì¼ë“¤ì„ ëª¨ë‘ ì½ì–´ì„œ í†µí•©ëœ ìƒí˜¸ì‘ìš© ë°ì´í„° ë°˜í™˜
    
    Returns:
        interactions_df: ì‚¬ìš©ì-ë‰´ìŠ¤ ìƒí˜¸ì‘ìš© DataFrame (user_id, news_id, strength)
    """
    import glob
    
    # data ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  interactions_*.csv íŒŒì¼ ì°¾ê¸°
    data_dir = Path(__file__).resolve().parent.parent.parent.parent / "data"
    csv_pattern = str(data_dir / "interactions_*.csv")
    csv_files = glob.glob(csv_pattern)
    
    # 14ì¼ì¹˜ â†’ 1ì¼ì¹˜ë¡œ ë³€ê²½: ê°€ì¥ ìµœì‹  íŒŒì¼ë§Œ ì‚¬ìš©
    if csv_files:
        # íŒŒì¼ëª…ìœ¼ë¡œ ì •ë ¬í•´ì„œ ê°€ì¥ ìµœì‹  íŒŒì¼ í•˜ë‚˜ë§Œ ì„ íƒ (interactions_YYYYMMDD.csv í˜•ì‹)
        csv_files = [sorted(csv_files)[-1]]
        print(f"[INFO] ìµœì‹  CSV íŒŒì¼ 1ê°œë§Œ ì‚¬ìš© (1ì¼ì¹˜ ë°ì´í„°)")
    
    if not csv_files:
        print(f"[ERROR] {data_dir}ì—ì„œ ìƒí˜¸ì‘ìš© CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return pd.DataFrame()
    
    print(f"[INFO] {len(csv_files)}ê°œ ìƒí˜¸ì‘ìš© CSV íŒŒì¼ ë°œê²¬:")
    for f in sorted(csv_files):
        file_size = Path(f).stat().st_size / (1024*1024)  # MB
        print(f"  - {Path(f).name} ({file_size:.1f}MB)")
    
    # ëª¨ë“  CSV íŒŒì¼ì„ ì½ì–´ì„œ í†µí•©
    dfs = []
    total_rows = 0
    
    print(f"[INFO] CSV íŒŒì¼ ë¡œë“œ ì¤‘...")
    for csv_file in tqdm(sorted(csv_files), desc="Loading CSV files"):
        try:
            print(f"  Loading {Path(csv_file).name}... ", end="")
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            # ì»¬ëŸ¼ëª… ì •ë¦¬ (BOM ì œê±°)
            df.columns = df.columns.str.strip()
            
            if not df.empty and all(col in df.columns for col in ['user_id', 'news_id', 'strength']):
                dfs.append(df)
                total_rows += len(df)
                print(f"OK ({len(df):,} interactions)")
            else:
                print(f"ERROR - Missing required columns")
                
        except Exception as e:
            print(f"ERROR - {e}")
    
    if not dfs:
        print("[ERROR] CSV íŒŒì¼ì—ì„œ ìœ íš¨í•œ ìƒí˜¸ì‘ìš© ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return pd.DataFrame()
    
    # ëª¨ë“  ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ì¤‘ë³µ ì œê±°
    print(f"[INFO] ë°ì´í„° í†µí•© ë° ì¤‘ë³µ ì œê±° ì¤‘...")
    combined_df = pd.concat(dfs, ignore_index=True)
    
    # ê°™ì€ user_id, news_id ì¡°í•©ì´ ìˆìœ¼ë©´ strength ê°’ì„ í•©ê³„ (ìƒí˜¸ì‘ìš© ëˆ„ì )
    print(f"[INFO] user_idì™€ news_idë¡œ ê·¸ë£¹í™” ì¤‘...")
    combined_df = combined_df.groupby(['user_id', 'news_id'], as_index=False)['strength'].sum()
    
    print(f"[INFO] ì´ ìƒí˜¸ì‘ìš© ë¡œë“œ: {total_rows:,}ê°œ")
    print(f"[INFO] ì¤‘ë³µ ì œê±° í›„: {len(combined_df):,}ê°œ")
    print(f"[INFO] ê³ ìœ  ì‚¬ìš©ì: {combined_df['user_id'].nunique():,}ëª…")
    print(f"[INFO] ê³ ìœ  ë‰´ìŠ¤: {combined_df['news_id'].nunique():,}ê°œ")
    print(f"[INFO] ìƒí˜¸ì‘ìš© ê°•ë„ ë²”ìœ„: {combined_df['strength'].min():.2f} ~ {combined_df['strength'].max():.2f}")
    
    return combined_df

def test_cbf_recommendations(recommendations: Dict, user_stats: Dict, interactions_df: pd.DataFrame, num_test_users: int = 3):
    """
    CBF ì¶”ì²œ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì¶œë ¥ (train_cf ìŠ¤íƒ€ì¼)
    
    Args:
        recommendations: ì‚¬ìš©ìë³„ ì¶”ì²œ ê²°ê³¼
        user_stats: ì‚¬ìš©ìë³„ í†µê³„ ì •ë³´
        interactions_df: ì›ë³¸ ìƒí˜¸ì‘ìš© ë°ì´í„°
        num_test_users: í…ŒìŠ¤íŠ¸í•  ì‚¬ìš©ì ìˆ˜
    """
    if not recommendations:
        print("[WARNING] í…ŒìŠ¤íŠ¸í•  ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ CBF ì¶”ì²œ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ({num_test_users}ëª…ì˜ ìƒ˜í”Œ ì‚¬ìš©ì)")
    print(f"{'='*60}")
    
    # ëœë¤í•˜ê²Œ ì‚¬ìš©ì ì„ íƒ
    import random
    all_users = list(recommendations.keys())
    sample_users = random.sample(all_users, min(num_test_users, len(all_users)))
    
    for idx, user_id in enumerate(sample_users, 1):
        user_recs = recommendations[user_id]
        stats = user_stats.get(user_id, {})
        
        print(f"\nğŸ“Š [{idx}] User ID: {user_id}")
        print(f"   ğŸ’¬ ì´ ìƒí˜¸ì‘ìš©: {stats.get('total_interactions', 0)}íšŒ")
        print(f"   ğŸ¯ ìƒí˜¸ì‘ìš© ë‰´ìŠ¤ ìˆ˜: {stats.get('interacted_news_count', 0)}ê°œ")
        print(f"   ğŸ“ˆ í‰ê·  ìƒí˜¸ì‘ìš© ê°•ë„: {stats.get('avg_strength', 0):.3f}")
        
        # ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ìƒí˜¸ì‘ìš©í•œ ë‰´ìŠ¤ë“¤ í‘œì‹œ
        user_interactions = interactions_df[interactions_df['user_id'] == user_id]
        if not user_interactions.empty:
            top_interactions = user_interactions.nlargest(3, 'strength')
            print(f"\n   ğŸ“– ìµœê³  ìƒí˜¸ì‘ìš© ë‰´ìŠ¤:")
            for i, (_, row) in enumerate(top_interactions.iterrows(), 1):
                print(f"      {i}. News ID {row['news_id']} (ê°•ë„: {row['strength']:.2f})")
        
        # CBF ì¶”ì²œ ê²°ê³¼
        print(f"\n   ğŸ¤– CBF ì¶”ì²œ ë‰´ìŠ¤ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜):")
        for i, rec in enumerate(user_recs[:5], 1):  # ìƒìœ„ 5ê°œ
            title_short = rec['title'][:45] + "..." if len(rec['title']) > 45 else rec['title']
            category_display = f"[{rec['category']}]" if rec['category'] else "[ê¸°íƒ€]"
            print(f"      {i}. {category_display} {title_short}")
            print(f"          â†’ ìœ ì‚¬ë„: {rec['score']:.4f} | News ID: {rec['news_id']}")
        
        print(f"   {'â”€'*50}")
    
    # ì „ì²´ í†µê³„
    total_users = len(recommendations)
    avg_recs_per_user = sum(len(recs) for recs in recommendations.values()) / total_users
    
    print(f"\nğŸ“ˆ CBF ëª¨ë¸ ì „ì²´ í†µê³„:")
    print(f"   â€¢ í”„ë¡œí•„ì´ ìƒì„±ëœ ì‚¬ìš©ì: {total_users:,}ëª…")
    print(f"   â€¢ ì‚¬ìš©ìë‹¹ í‰ê·  ì¶”ì²œ ìˆ˜: {avg_recs_per_user:.1f}ê°œ")
    print(f"   â€¢ ìœ ì‚¬ë„ ë²”ìœ„: {min([min([r['score'] for r in recs]) for recs in recommendations.values()]):.4f} ~ {max([max([r['score'] for r in recs]) for recs in recommendations.values()]):.4f}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œ ë¶„í¬ 
    all_categories = {}
    for recs in recommendations.values():
        for rec in recs[:10]:  # ìƒìœ„ 10ê°œë§Œ ë¶„ì„
            cat = rec['category'] or 'ê¸°íƒ€'
            all_categories[cat] = all_categories.get(cat, 0) + 1
    
    print(f"\nğŸ“Š ì¶”ì²œ ì¹´í…Œê³ ë¦¬ ë¶„í¬ (ìƒìœ„ 5ê°œ):")
    top_categories = sorted(all_categories.items(), key=lambda x: x[1], reverse=True)[:5]
    for cat, count in top_categories:
        percentage = (count / sum(all_categories.values())) * 100
        print(f"   â€¢ {cat}: {count:,}íšŒ ({percentage:.1f}%)")
    
    print(f"\nâœ… CBF ì¶”ì²œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

def main():
    """
    CBF ì¶”ì²œ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ë©”ì¸ í•¨ìˆ˜
    
    1. ì˜¤ëŠ˜ ì´ë¯¸ CBF í•™ìŠµì„ ì™„ë£Œí–ˆëŠ”ì§€ í™•ì¸
    2. BigQueryì—ì„œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„° ë¡œë“œ
    3. PostgreSQLì—ì„œ ë‰´ìŠ¤ ì„ë² ë”© ë°ì´í„° ë¡œë“œ
    4. ì‚¬ìš©ìë³„ í”„ë¡œí•„ ë²¡í„° ìƒì„±
    5. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ CBF ì¶”ì²œ ìƒì„±
    6. ëª¨ë¸ ê²°ê³¼ë¥¼ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥
    """
    print(f"[START] CBF ëª¨ë¸ í•™ìŠµ ì‹œì‘ - {datetime.now()}")
    
    # 1. ì´ë¯¸ ì˜¤ëŠ˜ CBF í•™ìŠµì„ ì™„ë£Œí–ˆëŠ”ì§€ í™•ì¸
    if already_trained_cbf_today():
        print("[SKIP] ì˜¤ëŠ˜ ì´ë¯¸ CBF ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 2. CSV íŒŒì¼ì—ì„œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„° ë¡œë“œ
    try:
        interactions_df = load_interactions_from_csv()
        print(f"[INFO] CSV íŒŒì¼ì—ì„œ {len(interactions_df)}ê°œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë¡œë“œ ì™„ë£¼")
        
        if interactions_df.empty:
            print("[SKIP] CBF í•™ìŠµì— ì‚¬ìš©í•  ìƒí˜¸ì‘ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
            
    except Exception as e:
        print(f"[ERROR] CSVì—ì„œ ìƒí˜¸ì‘ìš© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 3. PostgreSQLì—ì„œ ë‰´ìŠ¤ ì„ë² ë”© ë°ì´í„° ë¡œë“œ
    news_df, embeddings_matrix = load_news_embeddings()
    
    if news_df.empty or embeddings_matrix.size == 0:
        print("[SKIP] CBF í•™ìŠµì— ì‚¬ìš©í•  ë‰´ìŠ¤ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # 4. ì‚¬ìš©ìë³„ í”„ë¡œí•„ ë²¡í„° ìƒì„±
    user_profiles, user_stats = create_user_profiles(interactions_df, news_df, embeddings_matrix)
    
    if not user_profiles:
        print("[SKIP] CBF í•™ìŠµì„ ìœ„í•œ ì‚¬ìš©ì í”„ë¡œí•„ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return
    
    # 5. CBF ì¶”ì²œ ìƒì„±
    recommendations = generate_recommendations(user_profiles, news_df, embeddings_matrix, top_k=50)
    
    if not recommendations:
        print("[SKIP] ì¶”ì²œ ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return
    
    # 6. CBF ëª¨ë¸ ì €ì¥
    save_cbf_model(recommendations, user_profiles, user_stats, 
                   len(news_df), embeddings_matrix.shape[1])
    
    # 7. í…ŒìŠ¤íŠ¸ìš© ì¶”ì²œ ê²°ê³¼ ì¶œë ¥ (train_cf ìŠ¤íƒ€ì¼)
    test_cbf_recommendations(recommendations, user_stats, interactions_df, num_test_users=3)
    
    # 8. ì˜¤ëŠ˜ CBF í•™ìŠµ ì™„ë£Œ ìƒíƒœ ê¸°ë¡
    mark_cbf_trained_today()
    
    print(f"[SUCCESS] CBF ëª¨ë¸ í•™ìŠµ ì™„ë£Œ - {datetime.now()}")

if __name__ == "__main__":
    main()