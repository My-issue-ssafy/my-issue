from datetime import datetime, timedelta
from sqlalchemy import func
from app.db.models.news import News
from app.core.crawler.naver_crawler import (
    discover_and_store, discover_links, fetch_and_parse, embed_title, EMBED_MODEL_NAME, SID1_TO_SECTION
)
from app.utils.config import settings
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import random
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from loguru import logger

engine = create_engine(settings.DATABASE_URL, pool_pre_ping=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
MAX_PAGE=100

def save_news_to_db(article: dict, db):
    """í¬ë¡¤ë§ëœ ê¸°ì‚¬ -> DB ì €ìž¥"""
    try:
        content_blocks = article.get("body", [])
        category = ", ".join(article.get("category", []))
        created_at = (
            datetime.fromisoformat(article["published_at"])
            if article.get("published_at")
            else None
        )

        # created_atì´ Noneì´ë©´ DBì— ì €ìž¥í•˜ì§€ ì•ŠìŒ (ìŠ¤í¬ì¸ /ì—°ì˜ˆ ë‰´ìŠ¤ í•„í„°ë§)
        if created_at is None:
            logger.warning(f"[SKIP] created_atì´ Noneì¸ ê¸°ì‚¬ ìŠ¤í‚µ: {article.get('title', 'Unknown')}")
            return

        embedding = article.get("title_embedding", {}).get("vector")

        # âœ… ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        thumbnail_url = None
        if isinstance(content_blocks, list):
            for block in content_blocks:
                if block.get("type") == "image":
                    thumbnail_url = block.get("content")
                    break

        news = News(
            title=article.get("title"),
            content=content_blocks,
            category=category,
            author=article.get("reporter"),
            news_paper=article.get("press"),
            created_at=created_at,
            views=random.randint(1000, 10000),
            embedding=embedding,
            thumbnail=thumbnail_url,
            scrap_count=random.randint(1, 20)
        )
        db.add(news)
        db.commit()
        logger.info(f"[DB] saved: {news.title}")

    except Exception as e:
        db.rollback()
        logger.error(f"[DB ERROR] {e}")


def get_latest_times_per_section(db):
    """DBì—ì„œ ì„¹ì…˜ë³„ ìµœì‹  ìž‘ì„±ì‹œê°„ ê°€ì ¸ì˜¤ê¸°"""
    rows = (
        db.query(News.category, func.max(News.created_at))
        .group_by(News.category)
        .all()
    )
    return {row[0]: row[1] for row in rows if row[1]}
    
def crawl_section(sid1: str, reg_date: str, latest_times: dict, seen_ids_lock: threading.Lock, global_seen_ids: set[str]):
    """ë‹¨ì¼ ì„¹ì…˜ í¬ë¡¤ë§ (ìŠ¤ë ˆë“œìš©)"""
    section_name = SID1_TO_SECTION.get(sid1, "ê¸°íƒ€")
    latest_dt = latest_times.get(section_name)

    # ê° ìŠ¤ë ˆë“œë³„ë¡œ ë³„ë„ DB ì„¸ì…˜ ìƒì„±
    db = SessionLocal()
    local_seen_ids = set()

    try:
        logger.info(f"â–¶ [Thread-{sid1}] ì„¹ì…˜ {sid1} ({section_name}) í¬ë¡¤ë§ ì‹œìž‘")

        def thread_safe_save(article: dict, db_session):
            """ìŠ¤ë ˆë“œ ì•ˆì „í•œ ì €ìž¥ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ìš© - í”„ë¦°íŠ¸ë§Œ)"""
            canon_url = article.get("url")
            if not canon_url:
                return

            # ì „ì—­ seen_ids ì²´í¬ (ë½ ì‚¬ìš©)
            with seen_ids_lock:
                if canon_url in global_seen_ids:
                    return
                global_seen_ids.add(canon_url)

            # ë¡œì»¬ì—ë„ ì¶”ê°€
            local_seen_ids.add(canon_url)


            # DB ì €ìž¥
            try:
                save_news_to_db(article, db_session)
            except Exception as e:
                logger.error(f"    âŒ [Thread-{sid1}] DB ì €ìž¥ ì˜¤ë¥˜: {e}")

        discover_and_store(sid1, reg_date, db, latest_dt, local_seen_ids, thread_safe_save, max_pages=MAX_PAGE)

        logger.info(f"âœ” [Thread-{sid1}] ì„¹ì…˜ {sid1} ({section_name}) í¬ë¡¤ë§ ì™„ë£Œ - {len(local_seen_ids)}ê°œ ê¸°ì‚¬")
        return len(local_seen_ids)

    except Exception as e:
        logger.error(f"âŒ [Thread-{sid1}] ì„¹ì…˜ {sid1} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return 0
    finally:
        db.close()

def run_crawl_job():
    """ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ë§ ì‹¤í–‰"""
    today = datetime.today()
    reg_dates = [(today - timedelta(days=i)).strftime("%Y%m%d") for i in range(1)]

    # ë©”ì¸ DB ì„¸ì…˜ìœ¼ë¡œ ìµœì‹  ì‹œê°„ ì¡°íšŒ
    db = SessionLocal()
    latest_times = get_latest_times_per_section(db)
    db.close()

    logger.info(f"[LATEST] ì„¹ì…˜ë³„ ìµœì‹  ìž‘ì„±ì‹œê°„: {latest_times}")

    # ìŠ¤ë ˆë“œ ì•ˆì „í•œ ì „ì—­ seen_ids
    global_seen_ids: set[str] = set()
    seen_ids_lock = threading.Lock()

    total_articles = 0

    for reg in reg_dates:
        logger.info(f"\n=== ë‚ ì§œ: {reg} (ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ë§) ===")

        # ThreadPoolExecutorë¡œ 6ê°œ ì„¹ì…˜ ë™ì‹œ ì‹¤í–‰
        sections = ["100", "101", "102", "103", "104", "105"]

        with ThreadPoolExecutor(max_workers=6, thread_name_prefix="Crawler") as executor:
            # ê° ì„¹ì…˜ë³„ë¡œ ìŠ¤ë ˆë“œ ì‹œìž‘
            future_to_section = {
                executor.submit(crawl_section, sid1, reg, latest_times, seen_ids_lock, global_seen_ids): sid1
                for sid1 in sections
            }

            # ì™„ë£Œëœ ìŠ¤ë ˆë“œ ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_section):
                sid1 = future_to_section[future]
                try:
                    article_count = future.result()
                    total_articles += article_count
                except Exception as e:
                    logger.error(f"âŒ ì„¹ì…˜ {sid1} ìŠ¤ë ˆë“œ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    logger.info(f"\nðŸŽ‰ ===== ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ë§ ì™„ë£Œ! =====")
    logger.info(f"ðŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
    logger.info(f"  ðŸ“° ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {total_articles:,}ê°œ")
    logger.info(f"  ðŸ”— ê³ ìœ  URL: {len(global_seen_ids):,}ê°œ")
    logger.info(f"  ðŸ§µ ì‚¬ìš©ëœ ìŠ¤ë ˆë“œ: 6ê°œ (ì„¹ì…˜ë³„ ë³‘ë ¬ ì²˜ë¦¬)")
    logger.info(f"â° í¬ë¡¤ë§ ì™„ë£Œ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"==========================================\n")


# def run_crawl_job():
#     """í¬ë¡¤ë§ ì‹¤í–‰ â†’ PostgreSQL ì €ìž¥"""
#     today = datetime.today()
#     reg_dates = [(today - timedelta(days=i)).strftime("%Y%m%d") for i in range(1)]

#     db = SessionLocal()
#     latest_times = get_latest_times_per_section(db)
#     print(f"[LATEST] ì„¹ì…˜ë³„ ìµœì‹  ìž‘ì„±ì‹œê°„: {latest_times}")

#     seen_ids: set[str] = set()
#     for reg in reg_dates:
#         for sid1 in ["100", "101", "102", "103", "104", "105"]:
#             section_name = SID1_TO_SECTION.get(sid1, "ê¸°íƒ€")
#             stop_section = False

#             try:
#                 urls = discover_links(sid1, reg, max_pages=MAX_PAGE)
#             except Exception as e:
#                 print(f"[ERR] discover sid1={sid1}/{reg} -> {e}")
#                 continue

#             for u in urls:
#                 try:
#                     item = fetch_and_parse(u, sid1=sid1)
#                     if not item:
#                         continue

#                     pub_time = item.get("published_at")
#                     if pub_time:
#                         pub_dt = datetime.fromisoformat(pub_time)
#                         latest_dt = latest_times.get(section_name)

#                         if latest_dt and pub_dt <= latest_dt:
#                             print(f"[STOP] ì„¹ì…˜ {sid1} ({section_name}) "
#                                   f"- {pub_dt} <= {latest_dt}")
#                             stop_section = True
#                             break

#                     # ì œëª© ìž„ë² ë”© ì¶”ê°€
#                     try:
#                         emb = embed_title(item.get("title") or "")
#                         if emb:
#                             item["title_embedding"] = {
#                                 "model": EMBED_MODEL_NAME,
#                                 "vector": emb,
#                                 "dim": len(emb),
#                                 "normalized": True,
#                             }
#                     except Exception as e:
#                         print("[EMB-ERR] title embedding:", e)

#                     canon_url = item.get("url")
#                     if canon_url and canon_url in seen_ids:
#                         continue
#                     seen_ids.add(canon_url)

#                     save_news_to_db(item, db)

#                 except Exception as e:
#                     print("[ERR]", u, e)

#             if stop_section:
#                 print(f"[INFO] ì„¹ì…˜ {sid1} ({section_name}) í¬ë¡¤ë§ ì¤‘ë‹¨")
#                 continue

#     db.close()
#     print("âœ” í¬ë¡¤ë§ ì™„ë£Œ")
