#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (DB ì €ì¥ ì—†ì´ í”„ë¦°íŠ¸ë§Œ)
"""

import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
import sys
import os

# FastAPI ì•± ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), 'fastapi'))

from app.core.crawler.naver_crawler import (
    discover_links, fetch_and_parse, embed_title, EMBED_MODEL_NAME, SID1_TO_SECTION
)

MAX_TEST_PAGES = 2  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ê²Œ

def test_crawl_section(sid1: str, reg_date: str, seen_ids_lock: threading.Lock, global_seen_ids: set[str]):
    """í…ŒìŠ¤íŠ¸ìš© ë‹¨ì¼ ì„¹ì…˜ í¬ë¡¤ë§ (í”„ë¦°íŠ¸ë§Œ)"""
    section_name = SID1_TO_SECTION.get(sid1, "ê¸°íƒ€")
    local_seen_ids = set()

    try:
        print(f"ğŸš€ [Thread-{sid1}] ì„¹ì…˜ {sid1} ({section_name}) í¬ë¡¤ë§ ì‹œì‘")

        # ë§í¬ ìˆ˜ì§‘
        urls = discover_links(sid1, reg_date, max_pages=MAX_TEST_PAGES)
        print(f"ğŸ“‹ [Thread-{sid1}] ìˆ˜ì§‘ëœ ë§í¬: {len(urls)}ê°œ")

        for i, url in enumerate(urls[:10]):  # ìµœëŒ€ 10ê°œë§Œ í…ŒìŠ¤íŠ¸
            try:
                article = fetch_and_parse(url, sid1=sid1)
                if not article:
                    continue

                canon_url = article.get("url")
                if not canon_url:
                    continue

                # ì „ì—­ seen_ids ì²´í¬ (ë½ ì‚¬ìš©)
                with seen_ids_lock:
                    if canon_url in global_seen_ids:
                        continue
                    global_seen_ids.add(canon_url)

                local_seen_ids.add(canon_url)

                # í”„ë¦°íŠ¸
                title = article.get("title", "ì œëª© ì—†ìŒ")
                press = article.get("press", "ì–¸ë¡ ì‚¬ ì—†ìŒ")
                category = ", ".join(article.get("category", []))
                published = article.get("published_at", "ë‚ ì§œ ì—†ìŒ")

                # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°
                body = article.get("body", [])
                if isinstance(body, list) and body:
                    preview = body[0].get("content", "")[:50] + "..."
                else:
                    preview = str(body)[:50] + "..." if body else "ë³¸ë¬¸ ì—†ìŒ"

                print(f"  ğŸ“° [Thread-{sid1}][{i+1:2d}/10] {title}")
                print(f"      ğŸ¢ {press} | ğŸ“… {published[:19] if published else 'N/A'}")
                print(f"      ğŸ“ {preview}")
                print()

            except Exception as e:
                print(f"    âŒ [Thread-{sid1}] ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")

        print(f"âœ… [Thread-{sid1}] ì„¹ì…˜ {sid1} ({section_name}) ì™„ë£Œ - {len(local_seen_ids)}ê°œ ê¸°ì‚¬")
        return len(local_seen_ids)

    except Exception as e:
        print(f"ğŸ’¥ [Thread-{sid1}] ì„¹ì…˜ {sid1} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return 0

def test_multithread_crawling():
    """ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”¥ ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘!\n")

    today = datetime.today()
    reg_date = today.strftime("%Y%m%d")  # ì˜¤ëŠ˜ë§Œ

    # ìŠ¤ë ˆë“œ ì•ˆì „í•œ ì „ì—­ seen_ids
    global_seen_ids: set[str] = set()
    seen_ids_lock = threading.Lock()

    total_articles = 0

    print(f"ğŸ“… í…ŒìŠ¤íŠ¸ ë‚ ì§œ: {reg_date}")
    print(f"ğŸ“„ í˜ì´ì§€ ì œí•œ: {MAX_TEST_PAGES}í˜ì´ì§€")
    print(f"ğŸ”— ê¸°ì‚¬ ì œí•œ: ì„¹ì…˜ë‹¹ ìµœëŒ€ 10ê°œ")
    print("-" * 60)

    # 6ê°œ ì„¹ì…˜ ë™ì‹œ ì‹¤í–‰
    sections = ["100", "101", "102", "103", "104", "105"]

    start_time = datetime.now()

    with ThreadPoolExecutor(max_workers=6, thread_name_prefix="TestCrawler") as executor:
        # ê° ì„¹ì…˜ë³„ë¡œ ìŠ¤ë ˆë“œ ì‹œì‘
        future_to_section = {
            executor.submit(test_crawl_section, sid1, reg_date, seen_ids_lock, global_seen_ids): sid1
            for sid1 in sections
        }

        # ì™„ë£Œëœ ìŠ¤ë ˆë“œ ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(future_to_section):
            sid1 = future_to_section[future]
            try:
                article_count = future.result()
                total_articles += article_count
                section_name = SID1_TO_SECTION.get(sid1, "ê¸°íƒ€")
                print(f"ğŸ ì„¹ì…˜ {sid1} ({section_name}) ìŠ¤ë ˆë“œ ì™„ë£Œ!")
            except Exception as e:
                print(f"ğŸ’¥ ì„¹ì…˜ {sid1} ìŠ¤ë ˆë“œ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print("\n" + "=" * 60)
    print("ğŸ‰ ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {duration:.2f}ì´ˆ")
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {total_articles:,}ê°œ")
    print(f"ğŸ”— ê³ ìœ  URL: {len(global_seen_ids):,}ê°œ")
    print(f"ğŸ§µ ë™ì‹œ ìŠ¤ë ˆë“œ: 6ê°œ")

if __name__ == "__main__":
    test_multithread_crawling()